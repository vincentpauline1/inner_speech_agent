{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa1f0aae-0ad2-4a21-b363-e92dc7a1c0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current path: /mnt/c/Users/vpaul/OneDrive - CentraleSupelec/Inner_Speech/agent/temp_repo/imitative_agent\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as ipw\n",
    "import numpy as np \n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from imitative_agent import ImitativeAgent\n",
    "from lib.dataset_wrapper import Dataset\n",
    "from lib import utils\n",
    "from lib import abx_utils\n",
    "from lib import notebooks\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f5e19cb-778d-4ddb-b31d-d67db98a1bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get paths to all imitative agent directories and sort them\n",
    "agents_path = glob(\"../out/imitative_agent/*/\")\n",
    "agents_path.sort()\n",
    "\n",
    "# Dictionary to store agent aliases mapped to their paths\n",
    "agents_alias = {}\n",
    "# Dictionary to store groups of agents with same parameters\n",
    "agents_group = {}\n",
    "\n",
    "for agent_path in agents_path:\n",
    "    # Print current working directory and agent path for debugging\n",
    "    print(os.getcwd())\n",
    "    print(agent_path)\n",
    "    \n",
    "    # Load agent configuration without neural networks for efficiency\n",
    "    agent = ImitativeAgent.reload(agent_path, load_nn=False)\n",
    "    config = agent.config\n",
    "    \n",
    "    # Skip agents with different jerk loss ceiling (commented out)\n",
    "    #if config[\"training\"][\"jerk_loss_ceil\"] != 0.014: continue\n",
    "        \n",
    "    # Get agent identifier from path\n",
    "    agent_i = agent_path[-2]\n",
    "    \n",
    "    # Create descriptive alias string containing key agent parameters\n",
    "    agent_alias = \" \".join((\n",
    "        f\"{','.join(config['dataset']['names'])}\",  # Dataset names\n",
    "        f\"synth_art={agent.synthesizer.config['dataset']['art_type']}\", # Articulatory features type\n",
    "        f\"jerk_c={config['training']['jerk_loss_ceil']}\", # Jerk loss ceiling\n",
    "        f\"jerk_w={config['training']['jerk_loss_weight']}\", # Jerk loss weight\n",
    "        f\"bi={config['model']['inverse_model']['bidirectional']}\", # Whether inverse model is bidirectional\n",
    "        f\"({agent_i})\", # Agent identifier\n",
    "    ))\n",
    "    # Store mapping between alias and path\n",
    "    agents_alias[agent_alias] = agent_path\n",
    "    \n",
    "    # Create group string with same parameters but without agent identifier\n",
    "    agent_group = \" \".join((\n",
    "        f\"{','.join(config['dataset']['names'])}\",\n",
    "        f\"synth_art={agent.synthesizer.config['dataset']['art_type']}\",\n",
    "        f\"jerk_c={config['training']['jerk_loss_ceil']}\",\n",
    "        f\"jerk_w={config['training']['jerk_loss_weight']}\",\n",
    "        f\"bi={config['model']['inverse_model']['bidirectional']}\",\n",
    "    ))\n",
    "    # Initialize empty list for new group\n",
    "    if agent_group not in agents_group:\n",
    "        agents_group[agent_group] = []\n",
    "    # Add agent path to its parameter group    \n",
    "    agents_group[agent_group].append(agent_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46b907ea-5f85-4da3-9697-c4944563532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TONGUE_CONSONANTS = [\"p\", \"b\", \"t\", \"d\", \"k\", \"g\"]\n",
    "DETECTION_METHODS = {\n",
    "    \"p\": \"lips\",\n",
    "    \"b\": \"lips\",\n",
    "    \"t\": \"tongue_tip\",\n",
    "    \"d\": \"tongue_tip\",\n",
    "    \"k\": \"tongue_mid\",\n",
    "    \"g\": \"tongue_mid\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31c5fb8b-4cbf-47d1-bc6d-485a00b2edfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2d96898751414a860003bf70c535b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize dictionaries to store EMA data and occlusion information\n",
    "agents_ema = {}  # Maps agent paths to their EMA data\n",
    "datasets_occlusions = {}  # Maps dataset names to their occlusion data\n",
    "\n",
    "# Process each agent's data\n",
    "for agent_alias, agent_path in tqdm(agents_alias.items()):\n",
    "    # Initialize dictionary for this agent's EMA data\n",
    "    agent_ema = agents_ema[agent_path] = {}\n",
    "    \n",
    "    # Load agent and get its synthesizer dataset\n",
    "    agent = ImitativeAgent.reload(agent_path)\n",
    "    synth_dataset = agent.synthesizer.dataset\n",
    "    \n",
    "    # Get main dataset and features for all splits\n",
    "    main_dataset = agent.get_main_dataset()\n",
    "    agent_features = agent.repeat_datasplit(None)\n",
    "    \n",
    "    # Process each dataset used by this agent\n",
    "    for dataset_name, dataset_features in agent_features.items():\n",
    "        # Only compute occlusions once per dataset\n",
    "        if dataset_name not in datasets_occlusions:\n",
    "            # Load dataset and extract relevant information\n",
    "            dataset = Dataset(dataset_name)\n",
    "            palate = dataset.palate\n",
    "            vowels = dataset.phones_infos[\"vowels\"]  # Get vowels from phones_infos.yaml\n",
    "            datasets_lab = {dataset_name: dataset.lab}\n",
    "            datasets_ema = {dataset_name: dataset.get_items_data(\"ema\")}\n",
    "            \n",
    "            # Get indexes of consonant occurrences in the dataset\n",
    "            consonants_indexes = abx_utils.get_datasets_phones_indexes(\n",
    "                datasets_lab, TONGUE_CONSONANTS, vowels\n",
    "            )\n",
    "            # Detect occlusion points for each consonant using articulatory data\n",
    "            datasets_occlusions[dataset_name] = abx_utils.get_occlusions_indexes(\n",
    "                TONGUE_CONSONANTS, consonants_indexes, DETECTION_METHODS, datasets_ema, palate,\n",
    "            )\n",
    "        \n",
    "        # Initialize dictionary for estimated EMA data for this dataset\n",
    "        items_estimated_ema = agent_ema[dataset_name] = {}\n",
    "        \n",
    "        # Convert estimated articulatory features to EMA coordinates\n",
    "        items_estimated_art = dataset_features[\"art_estimated\"]\n",
    "        for item_name, item_estimated_art in items_estimated_art.items():\n",
    "            item_estimated_ema = synth_dataset.art_to_ema(item_estimated_art)\n",
    "            items_estimated_ema[item_name] = item_estimated_ema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "286e74fb-89f5-4446-9a44-70ab93fe95dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab5cd9d1275416f849187ca91c05187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='dataset_name', options=(), value=None), Output()), _dom_classes=('…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_dataset(dataset_name):\n",
    "    \"\"\"Interactive visualization of articulatory occlusions for consonant production.\n",
    "    \n",
    "    Displays scatter plots of articulator positions at consonant start and stop times,\n",
    "    with statistical measures of key articulatory features (lip aperture, tongue distances).\n",
    "    \n",
    "    Args:\n",
    "        dataset_name (str): Name of dataset to analyze\n",
    "    \"\"\"\n",
    "    # Load dataset and extract relevant information\n",
    "    dataset = Dataset(dataset_name)\n",
    "    items_ema = dataset.get_items_data(\"ema\")\n",
    "    dataset_occlusions = datasets_occlusions[dataset_name]\n",
    "    palate = dataset.palate\n",
    "    \n",
    "    # Set display boundaries with padding\n",
    "    display_xlim = (dataset.ema_limits[\"xmin\"] * 0.95, dataset.ema_limits[\"xmax\"] * 1.05)\n",
    "    display_ylim = (dataset.ema_limits[\"ymin\"] * 0.95, dataset.ema_limits[\"ymax\"] * 1.05)\n",
    "    \n",
    "    def show_occlusions(offset=2):\n",
    "        \"\"\"Plot occlusion patterns for each consonant with configurable time offset.\n",
    "        \n",
    "        Args:\n",
    "            offset (int): Number of frames to offset from detected occlusion points\n",
    "        \"\"\"\n",
    "        consonants_stats = {}\n",
    "        \n",
    "        for consonant, occlusions in dataset_occlusions.items():\n",
    "            # Create figure with two subplots for start and stop positions\n",
    "            plt.figure(figsize=(12, 3), dpi=60)\n",
    "            \n",
    "            # Configure start position subplot\n",
    "            ax_start = plt.subplot(121, aspect=\"equal\")\n",
    "            ax_start.set_title(f\"{consonant} start (PB original)\", pad=10)\n",
    "            ax_start.set_xlim(*display_xlim)\n",
    "            ax_start.set_ylim(*display_ylim)\n",
    "            ax_start.plot(palate[:, 0], palate[:, 1], 'k-', linewidth=2, label='Palate')\n",
    "            ax_start.set_xticks([])\n",
    "            ax_start.set_yticks([])\n",
    "            ax_start.grid(True, alpha=0.3)\n",
    "\n",
    "            # Configure stop position subplot  \n",
    "            ax_stop = plt.subplot(122, aspect=\"equal\")\n",
    "            ax_stop.set_title(f\"{consonant} stop (PB original)\", pad=10)\n",
    "            ax_stop.set_xlim(*display_xlim)\n",
    "            ax_stop.set_ylim(*display_ylim)\n",
    "            ax_stop.plot(palate[:, 0], palate[:, 1], 'k-', linewidth=2, label='Palate')\n",
    "            ax_stop.set_xticks([])\n",
    "            ax_stop.set_yticks([])\n",
    "            ax_stop.grid(True, alpha=0.3)\n",
    "\n",
    "            # Extract EMA positions at occlusion times\n",
    "            occlusions_start_ema = []\n",
    "            occlusions_stop_ema = []\n",
    "            for occlusion in occlusions:\n",
    "                item_ema = items_ema[occlusion[1]]\n",
    "                occlusions_start_ema.append(item_ema[occlusion[2] - offset])\n",
    "                occlusions_stop_ema.append(item_ema[occlusion[3] + offset])\n",
    "            occlusions_start_ema = np.array(occlusions_start_ema)\n",
    "            occlusions_stop_ema = np.array(occlusions_stop_ema) \n",
    "            \n",
    "            # Calculate articulatory statistics\n",
    "            occlusions_stats = consonants_stats[consonant] = {}\n",
    "            for occlusions_type, occlusions_ema in {\"start\": occlusions_start_ema, \"stop\": occlusions_stop_ema}.items():\n",
    "                # Measure lip aperture\n",
    "                lips_distance = np.sqrt(np.sum((occlusions_ema[:, 10:12] - occlusions_ema[:, 8:10]) ** 2, axis=1))\n",
    "                occlusions_stats[f\"{occlusions_type}_lips\"] = f\"{lips_distance.mean():.2f} ±{lips_distance.std():.2f}\"\n",
    "                \n",
    "                # Measure tongue tip to palate distance\n",
    "                tongue_tip_distance = abx_utils.coil_distances_from_palate(occlusions_ema[:, 2:4], palate)\n",
    "                occlusions_stats[f\"{occlusions_type}_tongue_tip\"] = f\"{tongue_tip_distance.mean():.2f} ±{tongue_tip_distance.std():.2f}\"\n",
    "                \n",
    "                # Measure tongue mid to palate distance\n",
    "                tongue_mid_distance = abx_utils.coil_distances_from_palate(occlusions_ema[:, 4:6], palate)\n",
    "                occlusions_stats[f\"{occlusions_type}_tongue_mid\"] = f\"{tongue_mid_distance.mean():.2f} ±{tongue_mid_distance.std():.2f}\"\n",
    "\n",
    "            # Plot articulator positions\n",
    "            ax_start.scatter(occlusions_start_ema[:, 0::2], occlusions_start_ema[:, 1::2], \n",
    "                           c=\"tab:blue\", s=15, alpha=0.6, label='Articulators')\n",
    "            ax_stop.scatter(occlusions_stop_ema[:, 0::2], occlusions_stop_ema[:, 1::2], \n",
    "                          c=\"tab:blue\", s=15, alpha=0.6, label='Articulators')\n",
    "\n",
    "            ax_start.legend(loc='upper right')\n",
    "            ax_stop.legend(loc='upper right')\n",
    "            plt.subplots_adjust(wspace=-.1)\n",
    "            plt.show()\n",
    "            \n",
    "        # Display statistical summary\n",
    "        consonants_stats = pd.DataFrame.from_dict(consonants_stats, orient=\"index\")\n",
    "        display(consonants_stats)\n",
    "    \n",
    "    # Create interactive offset control\n",
    "    ipw.interact(show_occlusions, offset=(0, 10))\n",
    "\n",
    "# Create dataset selector dropdown\n",
    "ipw.interactive(show_dataset, dataset_name=datasets_occlusions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e22e4695-2f9d-4c6e-a234-bb55a677660e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ab85f2fee34146877954412ede9665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='agent_alias', options=(), value=None), Output()), _dom_classes=('w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_agent(agent_alias):\n",
    "    \"\"\"Interactive visualization of articulatory occlusions for agent's consonant production.\n",
    "    \n",
    "    Displays scatter plots of articulator positions at consonant start and stop times,\n",
    "    with statistical measures of key articulatory features (lip aperture, tongue distances).\n",
    "    \n",
    "    Args:\n",
    "        agent_alias (str): Alias name of agent to analyze\n",
    "    \"\"\"\n",
    "    # Load agent and extract relevant information\n",
    "    agent_path = agents_alias[agent_alias]\n",
    "    agent = ImitativeAgent.reload(agent_path, load_nn=False)\n",
    "    synth_dataset = agent.synthesizer.dataset\n",
    "    palate = synth_dataset.palate\n",
    "    agent_ema = agents_ema[agent_path]\n",
    "    \n",
    "    # Set display boundaries with padding\n",
    "    display_xlim = (synth_dataset.ema_limits[\"xmin\"] * 0.95, synth_dataset.ema_limits[\"xmax\"] * 1.05)\n",
    "    display_ylim = (synth_dataset.ema_limits[\"ymin\"] * 0.95, synth_dataset.ema_limits[\"ymax\"] * 1.05)\n",
    "    \n",
    "    def show_occlusions(offset=2):\n",
    "        \"\"\"Plot occlusion patterns for each consonant with configurable time offset.\n",
    "        \n",
    "        Args:\n",
    "            offset (int): Number of frames to offset from detected occlusion points\n",
    "        \"\"\"\n",
    "        consonants_stats = {}\n",
    "    \n",
    "        for dataset_name in agent.config[\"dataset\"][\"names\"]:\n",
    "            dataset = Dataset(dataset_name)\n",
    "            items_ema = agent_ema[dataset_name]\n",
    "            dataset_occlusions = datasets_occlusions[dataset_name]\n",
    "\n",
    "            for consonant, occlusions in dataset_occlusions.items():\n",
    "                # Create figure with two subplots for start and stop positions\n",
    "                plt.figure(figsize=(12, 3), dpi=60)\n",
    "\n",
    "                # Configure start position subplot\n",
    "                ax_start = plt.subplot(121, aspect=\"equal\")\n",
    "                ax_start.set_title(f\"{consonant} start (jerk={agent.config['training']['jerk_loss_weight']})\", pad=10)\n",
    "                ax_start.set_xlim(*display_xlim)\n",
    "                ax_start.set_ylim(*display_ylim)\n",
    "                ax_start.plot(palate[:, 0], palate[:, 1], 'k-', linewidth=2, label='Palate')\n",
    "                ax_start.set_xticks([])\n",
    "                ax_start.set_yticks([])\n",
    "                ax_start.grid(True, alpha=0.3)\n",
    "\n",
    "                # Configure stop position subplot\n",
    "                ax_stop = plt.subplot(122, aspect=\"equal\")\n",
    "                ax_stop.set_title(f\"{consonant} stop (jerk={agent.config['training']['jerk_loss_weight']})\", pad=10)\n",
    "                ax_stop.set_xlim(*display_xlim)\n",
    "                ax_stop.set_ylim(*display_ylim)\n",
    "                ax_stop.plot(palate[:, 0], palate[:, 1], 'k-', linewidth=2, label='Palate')\n",
    "                ax_stop.set_xticks([])\n",
    "                ax_stop.set_yticks([])\n",
    "                ax_stop.grid(True, alpha=0.3)\n",
    "\n",
    "                # Extract EMA positions at occlusion times\n",
    "                occlusions_start_ema = []\n",
    "                occlusions_stop_ema = []\n",
    "                for occlusion in occlusions:\n",
    "                    item_ema = items_ema[occlusion[1]]\n",
    "                    occlusions_start_ema.append(item_ema[occlusion[2] - offset])\n",
    "                    occlusions_stop_ema.append(item_ema[occlusion[3] + offset])\n",
    "                occlusions_start_ema = np.array(occlusions_start_ema)\n",
    "                occlusions_stop_ema = np.array(occlusions_stop_ema) \n",
    "                \n",
    "                # Calculate articulatory statistics\n",
    "                occlusions_stats = consonants_stats[consonant] = {}\n",
    "                for occlusions_type, occlusions_ema in {\"start\": occlusions_start_ema, \"stop\": occlusions_stop_ema}.items():\n",
    "                    # Measure lip aperture\n",
    "                    lips_distance = np.sqrt(np.sum((occlusions_ema[:, 10:12] - occlusions_ema[:, 8:10]) ** 2, axis=1))\n",
    "                    occlusions_stats[f\"{occlusions_type}_lips\"] = f\"{lips_distance.mean():.2f} ±{lips_distance.std():.2f}\"\n",
    "\n",
    "                    # Measure tongue tip to palate distance\n",
    "                    tongue_tip_distance = abx_utils.coil_distances_from_palate(occlusions_ema[:, 2:4], palate)\n",
    "                    occlusions_stats[f\"{occlusions_type}_tongue_tip\"] = f\"{tongue_tip_distance.mean():.2f} ±{tongue_tip_distance.std():.2f}\"\n",
    "\n",
    "                    # Measure tongue mid to palate distance\n",
    "                    tongue_mid_distance = abx_utils.coil_distances_from_palate(occlusions_ema[:, 4:6], palate)\n",
    "                    occlusions_stats[f\"{occlusions_type}_tongue_mid\"] = f\"{tongue_mid_distance.mean():.2f} ±{tongue_mid_distance.std():.2f}\"\n",
    "\n",
    "                # Plot articulator positions\n",
    "                ax_start.scatter(occlusions_start_ema[:, 0::2], occlusions_start_ema[:, 1::2], \n",
    "                               c=\"tab:blue\", s=15, alpha=0.6, label='Articulators')\n",
    "                ax_stop.scatter(occlusions_stop_ema[:, 0::2], occlusions_stop_ema[:, 1::2], \n",
    "                              c=\"tab:blue\", s=15, alpha=0.6, label='Articulators')\n",
    "\n",
    "                ax_start.legend(loc='upper right')\n",
    "                ax_stop.legend(loc='upper right')\n",
    "                plt.subplots_adjust(wspace=-.1)\n",
    "                plt.show()\n",
    "                 \n",
    "        # Display statistical summary\n",
    "        consonants_stats = pd.DataFrame.from_dict(consonants_stats, orient=\"index\")\n",
    "        display(consonants_stats)\n",
    "    \n",
    "    # Create interactive offset control\n",
    "    ipw.interact(show_occlusions, offset=(0, 10))\n",
    "\n",
    "# Create agent selector dropdown\n",
    "ipw.interactive(show_agent, agent_alias=sorted(agents_alias.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47802e7-cd60-4116-8dd5-a21e5af237b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
