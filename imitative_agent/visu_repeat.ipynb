{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffa60e6b-f469-470a-a8fb-069cd0085c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current path: /mnt/c/Users/vpaul/OneDrive - CentraleSupelec/Inner_Speech/agent/imitative_agent\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as ipw\n",
    "from IPython.display import Audio\n",
    "import numpy as np \n",
    "import pickle\n",
    "\n",
    "from imitative_agent import ImitativeAgent\n",
    "from lib.dataset_wrapper import Dataset\n",
    "from lib.notebooks import show_ema\n",
    "from external import lpcynet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3b8a943-a83a-40fc-9089-55ed02c1a87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'batch_size': 8, 'datasplits_size': [64, 16, 20], 'names': ['pb2007'], 'num_workers': 6, 'shuffle_between_epochs': True, 'sound_type': 'cepstrum'}, 'model': {'direct_model': {'activation': 'relu', 'batch_norm': True, 'dropout_p': 0.25, 'hidden_layers': [256, 256, 256, 256]}, 'inverse_model': {'bidirectional': True, 'dropout_p': 0.25, 'hidden_size': 32, 'num_layers': 2}}, 'synthesizer': {'name': 'ea587b76c95fecef01cfd16c7f5f289d-0/'}, 'training': {'jerk_loss_ceil': 0.014, 'jerk_loss_weight': 1, 'vel_loss_ceil': 0.014, 'vel_loss_weight': 1, 'learning_rate': 0.001, 'max_epochs': 500, 'patience': 25}}\n",
      "{'direct_model': {'activation': 'relu', 'batch_norm': True, 'dropout_p': 0.25, 'hidden_layers': [256, 256, 256, 256]}, 'inverse_model': {'bidirectional': True, 'dropout_p': 0.25, 'hidden_size': 32, 'num_layers': 2}}\n",
      "{'direct_model': {'activation': 'relu', 'batch_norm': True, 'dropout_p': 0.25, 'hidden_layers': [256, 256, 256, 256]}, 'inverse_model': {'bidirectional': True, 'dropout_p': 0.25, 'hidden_size': 32, 'num_layers': 2}}\n"
     ]
    }
   ],
   "source": [
    "# Get paths to all imitative agent in specific directory\n",
    "agents_path = glob(\"../out/imitative_agent1/*/\")\n",
    "agents_path.sort()\n",
    "\n",
    "print(f\"Found {len(agents_path)} agents\")\n",
    "\n",
    "# Dictionary to store agent aliases mapped to their paths\n",
    "agents_alias = {}\n",
    "\n",
    "for agent_path in agents_path:\n",
    "    # Load agent configuration without neural networks for efficiency\n",
    "    agent = ImitativeAgent.reload(agent_path, load_nn=False)\n",
    "    config = agent.config\n",
    "        \n",
    "    # Get agent identifier from path\n",
    "    agent_i = agent_path[-2]\n",
    "    \n",
    "    # Create descriptive alias string containing key agent parameters\n",
    "    agent_alias = \" \".join((\n",
    "        f\"{','.join(config['dataset']['names'])}\",  # Dataset names\n",
    "        f\"synth_art={agent.synthesizer.config['dataset']['art_type']}\", # Articulatory features type\n",
    "        f\"jerk_c={config['training']['jerk_loss_ceil']}\", # Jerk loss ceiling\n",
    "        f\"jerk_w={config['training']['jerk_loss_weight']}\", # Jerk loss weight\n",
    "        f\"bi={config['model']['inverse_model']['bidirectional']}\", # Bidirectional model flag\n",
    "        f\"({agent_i})\", # Agent identifier\n",
    "    ))\n",
    "\n",
    "    # Print agent information\n",
    "    print(f\"\\nPath: {agent_path}:\")\n",
    "    print(f\"- Datasets: {config['dataset']['names']}\")\n",
    "    print(f\"- Synthesizer art type: {agent.synthesizer.config['dataset']['art_type']}\")\n",
    "    print(f\"- Jerk loss ceiling: {config['training']['jerk_loss_ceil']}\")\n",
    "    print(f\"- Jerk loss weight: {config['training']['jerk_loss_weight']}\")\n",
    "    print(f\"- Bidirectional: {config['model']['inverse_model']['bidirectional']}\")\n",
    "    \n",
    "    # Store mapping between alias and path\n",
    "    agents_alias[agent_alias] = agent_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c686f786-c983-41a0-a544-270982879cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f96a2750a704dc58af9cbb39003c672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='agent_alias', options=('pb2007 synth_art=art_params jerk_c=0.014 j…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dictionary to store current item for each dataset\n",
    "datasets_current_item = {}\n",
    "\n",
    "def show_agent(agent_alias):\n",
    "    \"\"\"\n",
    "    Creates an interactive visualization for analyzing an imitative agent's speech synthesis and articulation.\n",
    "    \n",
    "    Args:\n",
    "        agent_alias (str): Alias identifying the agent to visualize\n",
    "        \n",
    "    Returns:\n",
    "        Interactive widget displaying audio and visualizations of speech repetition\n",
    "    \"\"\"\n",
    "    # Load the agent and get key configuration parameters\n",
    "    agent_path = agents_alias[agent_alias]\n",
    "    agent = ImitativeAgent.reload(agent_path)\n",
    "    \n",
    "    sound_type = agent.config[\"dataset\"][\"sound_type\"]\n",
    "    art_type = agent.synthesizer.config[\"dataset\"][\"art_type\"]\n",
    "    synth_dataset = agent.synthesizer.dataset\n",
    "    \n",
    "    def show_dataset(dataset_name):\n",
    "        \"\"\"\n",
    "        Creates interactive visualization for a specific dataset.\n",
    "        \n",
    "        Args:\n",
    "            dataset_name (str): Name of the dataset to visualize\n",
    "        \"\"\"\n",
    "        # Load dataset and extract features\n",
    "        dataset = Dataset(dataset_name)\n",
    "        items_cepstrum = dataset.get_items_data(sound_type, cut_silences=True)\n",
    "        items_source = dataset.get_items_data(\"source\", cut_silences=True)\n",
    "        sampling_rate = dataset.features_config[\"wav_sampling_rate\"]\n",
    "        \n",
    "        items_ema = dataset.get_items_data(\"ema\", cut_silences=True)\n",
    "        \n",
    "        # Get list of items and set current item\n",
    "        items_name = dataset.get_items_list()\n",
    "        if dataset_name in datasets_current_item:\n",
    "            current_item = datasets_current_item[dataset_name]\n",
    "        else:\n",
    "            current_item = items_name[0][1]\n",
    "        \n",
    "        def resynth_item(item_name=current_item, freeze_source=False):\n",
    "            \"\"\"\n",
    "            Resynthesize and visualize a specific utterance, showing original, repeated and estimated versions.\n",
    "            \n",
    "            Args:\n",
    "                item_name (str): Name of the utterance to process\n",
    "                freeze_source (bool): Whether to freeze the source features\n",
    "            \"\"\"\n",
    "            # Store current item for this dataset\n",
    "            datasets_current_item[dataset_name] = item_name\n",
    "            \n",
    "            # Load item data\n",
    "            item_cepstrum = items_cepstrum[item_name]\n",
    "            item_source = items_sources[item_name]\n",
    "            item_wave = dataset.get_item_wave(item_name)\n",
    "            nb_frames = len(item_cepstrum)\n",
    "            \n",
    "            # Generate repetition using the agent\n",
    "            repetition = agent.repeat(item_cepstrum)\n",
    "            repeated_cepstrum = repetition[\"sound_repeated\"]    # Via synthesizer\n",
    "            estimated_cepstrum = repetition[\"sound_estimated\"]  # Via direct model\n",
    "            estimated_art = repetition[\"art_estimated\"]\n",
    "            \n",
    "            # Optionally freeze source features\n",
    "            if freeze_source:\n",
    "                item_source[:] = (1, 0)\n",
    "            \n",
    "            # Combine cepstral coefficients with source features for synthesis\n",
    "            repeated_sound = np.concatenate((repeated_cepstrum, item_source), axis=1)\n",
    "            estimated_sound = np.concatenate((estimated_cepstrum, item_source), axis=1)\n",
    "\n",
    "            # Convert to waveforms using LPCNet\n",
    "            repeated_wave = lpcynet.synthesize_frames(repeated_sound)\n",
    "            estimated_wave = lpcynet.synthesize_frames(estimated_sound)\n",
    "            \n",
    "            # Display audio players for comparison\n",
    "            print(\"Original sound:\")\n",
    "            display(Audio(item_wave, rate=sampling_rate))\n",
    "            print(\"Repetition (Inverse model → Synthesizer → LPCNet):\")\n",
    "            display(Audio(repeated_wave, rate=sampling_rate))\n",
    "            print(\"Estimation (Inverse model → Direct model → LPCNet):\")\n",
    "            display(Audio(estimated_wave, rate=sampling_rate))\n",
    "            \n",
    "            # Create spectrogram visualizations\n",
    "            plt.figure(figsize=(nb_frames/20, 6), dpi=120)\n",
    "            \n",
    "            ax = plt.subplot(311)\n",
    "            ax.set_title(\"original %s\" % (sound_type))\n",
    "            ax.imshow(item_cepstrum.T, origin=\"lower\")\n",
    "            \n",
    "            ax = plt.subplot(312)\n",
    "            ax.set_title(\"Repetition\")\n",
    "            ax.imshow(repeated_cepstrum.T, origin=\"lower\")\n",
    "            \n",
    "            ax = plt.subplot(313)\n",
    "            ax.set_title(\"Estimation\")\n",
    "            ax.imshow(estimated_cepstrum.T, origin=\"lower\")\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Convert articulatory parameters to EMA coordinates if needed\n",
    "            if art_type == \"art_params\":\n",
    "                estimated_art = dataset.art_to_ema(estimated_art)\n",
    "            item_ema = items_ema[item_name]\n",
    "            show_ema(estimated_art, reference=item_ema, dataset=synth_dataset)\n",
    "        \n",
    "        # Create interactive widget for utterance selection\n",
    "        display(ipw.interactive(resynth_item, item_name=items_name, freeze_source=False))\n",
    "    \n",
    "    # Create interactive widget for dataset selection  \n",
    "    display(ipw.interactive(show_dataset, dataset_name=agent.config[\"dataset\"][\"names\"]))\n",
    "\n",
    "# Create top-level interactive widget for agent selection\n",
    "display(ipw.interactive(show_agent, agent_alias=sorted(agents_alias.keys())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
