{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2726a691-b2ec-4bd5-a117-59b43b79d240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current path: /mnt/c/Users/vpaul/OneDrive - CentraleSupelec/Inner_Speech/agent/communicative_agent\n",
      "current path: /mnt/c/Users/vpaul/OneDrive - CentraleSupelec/Inner_Speech/agent/communicative_agent\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "sys.path.append(\"..\")\n",
    "print(\"current path:\", os.getcwd())\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as ipw\n",
    "from IPython.display import Audio\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from communicative_agent import CommunicativeAgent\n",
    "from lib.dataset_wrapper import Dataset\n",
    "from lib.notebooks import show_ema\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "print(\"current path:\", os.getcwd())\n",
    "# sys.path.insert(0, \"/Users/ladislas/Desktop/motor_control_agent\")\n",
    "sys.path.insert(0, \"/mnt/c/Users/vpaul/Documents/Inner_Speech/agent/\")\n",
    "from external import lpcynet\n",
    "#from external import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53e8f20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 agents\n",
      "\n",
      "Path: ../out/communicative_agent/5e95a73fee5902e08b8838c6778b10dd-0/:\n",
      "- Datasets: ['pb2007']\n",
      "- Synthesizer art type: art_params\n",
      "- Jerk loss weight: 0.15\n",
      "\n",
      "Path: ../out/communicative_agent/732af0286099098cd5488228100d9cb1-0/:\n",
      "- Datasets: ['pb2007']\n",
      "- Synthesizer art type: art_params\n",
      "- Jerk loss weight: 0.15\n",
      "\n",
      "Path: ../out/communicative_agent/884b552070dd7a21e9901ec1cdb5a1e5-0/:\n",
      "- Datasets: ['pb2007']\n",
      "- Synthesizer art type: art_params\n",
      "- Jerk loss weight: 0.1\n",
      "\n",
      "Path: ../out/communicative_agent/9be83c9471e7f0e7ec19c4bd0ac540f7-0/:\n",
      "- Datasets: ['pb2007']\n",
      "- Synthesizer art type: art_params\n",
      "- Jerk loss weight: 0.15\n",
      "\n",
      "Path: ../out/communicative_agent/cca7402d9866782e2bc60b6c2cffc9c5-0/:\n",
      "- Datasets: ['pb2007']\n",
      "- Synthesizer art type: art_params\n",
      "- Jerk loss weight: 0.1\n",
      "\n",
      "Path: ../out/communicative_agent/d1e8dc5c1a2b0275d89ac357066fa2d0-0/:\n",
      "- Datasets: ['pb2007']\n",
      "- Synthesizer art type: art_params\n",
      "- Jerk loss weight: 0.1\n"
     ]
    }
   ],
   "source": [
    "agents_path = glob(\"../out/communicative_agent/*/\")\n",
    "agents_path.sort()\n",
    "\n",
    "print(f\"Found {len(agents_path)} agents\")\n",
    "\n",
    "# Dictionary to store agent aliases mapped to their paths\n",
    "agents_alias = {}\n",
    "\n",
    "for agent_path in agents_path:\n",
    "    # Load agent configuration without neural networks for efficiency\n",
    "    agent = CommunicativeAgent.reload(agent_path, load_nn=False)\n",
    "    config = agent.config\n",
    "        \n",
    "    # Get agent identifier from path\n",
    "    agent_i = agent_path[-2] \n",
    "    \n",
    "    # Handle nb_derivatives, always equal to 0 in our implementation\n",
    "    try:\n",
    "        nb_derivatives = config['model']['direct_model']['nb_derivatives']\n",
    "    except:\n",
    "        nb_derivatives = 0\n",
    "        \n",
    "    # Create descriptive alias string containing key agent parameters\n",
    "    agent_alias = \" \".join((\n",
    "        f\"{','.join(config['dataset']['names'])}\",  # Dataset names\n",
    "        f\"synth_art={agent.synthesizer.config['dataset']['art_type']}\", # Articulatory features type\n",
    "        f\"nd={nb_derivatives}\", # Number of derivatives\n",
    "        f\"jerk={config['training']['jerk_loss_weight']}\", # Jerk loss weight\n",
    "        f\"({agent_i})\", # Agent identifier\n",
    "    ))\n",
    "    \n",
    "    # Print agent information\n",
    "    print(f\"\\nPath: {agent_path}:\")\n",
    "    print(f\"- Datasets: {config['dataset']['names']}\")\n",
    "    print(f\"- Synthesizer art type: {agent.synthesizer.config['dataset']['art_type']}\")\n",
    "    print(f\"- Jerk loss weight: {config['training']['jerk_loss_weight']}\")\n",
    "    \n",
    "    # Store mapping between alias and path\n",
    "    agents_alias[agent_alias] = agent_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6638c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe5a247552d4011ab4d92a6b9a86b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='agent_path', options={'pb2007 synth_art=art_params nd=0 jerk=0.15 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_agent(agent_path):\n",
    "    \"\"\"\n",
    "    Creates an interactive visualization for analyzing a communicative agent's speech synthesis and articulation.\n",
    "    \n",
    "    Args:\n",
    "        agent_path (str): Path to the saved agent model\n",
    "        \n",
    "    Returns:\n",
    "        Interactive widget displaying audio and visualizations of speech repetition\n",
    "    \"\"\"\n",
    "    # Load the agent and get key configuration parameters\n",
    "    agent = CommunicativeAgent.reload(agent_path)\n",
    "    sound_type = agent.synthesizer.config[\"dataset\"][\"sound_type\"]\n",
    "    art_type = agent.synthesizer.config[\"dataset\"][\"art_type\"] \n",
    "    synth_dataset = agent.synthesizer.dataset\n",
    "    \n",
    "    def show_dataset(dataset_name):\n",
    "        \"\"\"\n",
    "        Creates interactive visualization for a specific dataset.\n",
    "        \n",
    "        Args:\n",
    "            dataset_name (str): Name of the dataset to visualize\n",
    "        \"\"\"\n",
    "        dataset = Dataset(dataset_name)\n",
    "        \n",
    "        # Load dataset and extract features\n",
    "        items_cepstrum = dataset.get_items_data(sound_type, cut_silences=False)\n",
    "        items_source = dataset.get_items_data(\"source\", cut_silences=False)\n",
    "        sampling_rate = dataset.features_config[\"wav_sampling_rate\"]\n",
    "        \n",
    "        # items_ema = dataset.get_items_data(\"ema\", cut_silences=True)\n",
    "        \n",
    "        items_name = dataset.get_items_list()\n",
    "        \n",
    "        def resynth_item(item_name):\n",
    "            \"\"\"\n",
    "            Resynthesize and visualize a specific utterance, showing original, repeated and estimated versions.\n",
    "            \n",
    "            Args:\n",
    "                item_name (str): Name of the utterance to process\n",
    "            \"\"\"\n",
    "            # Clear any existing plots\n",
    "            plt.close('all')\n",
    "            \n",
    "            # Load item data\n",
    "            item_cepstrum = items_cepstrum[item_name]\n",
    "            item_source = items_source[item_name]\n",
    "            item_wave = dataset.get_item_wave(item_name)\n",
    "            nb_frames = len(item_cepstrum)\n",
    "            \n",
    "            # Generate repetition using the agent\n",
    "            repetition = agent.repeat(item_cepstrum)\n",
    "            repeated_cepstrum = repetition[\"sound_repeated\"]    # Via synthesizer\n",
    "            estimated_cepstrum = repetition[\"sound_estimated\"]  # Via direct model\n",
    "            estimated_art = repetition[\"art_estimated\"]\n",
    "            \n",
    "            # Combine cepstral coefficients with source features for synthesis\n",
    "            repeated_sound = np.concatenate((repeated_cepstrum, item_source), axis=1)\n",
    "            estimated_sound = np.concatenate((estimated_cepstrum, item_source), axis=1)\n",
    "\n",
    "            # Convert to waveforms using LPCNet\n",
    "            repeated_wave = lpcynet.synthesize_frames(repeated_sound)\n",
    "            estimated_wave = lpcynet.synthesize_frames(estimated_sound)\n",
    "            \n",
    "            # Display audio players for comparison\n",
    "            print(\"Original sound:\")\n",
    "            display(Audio(item_wave, rate=sampling_rate))\n",
    "            print(\"Repetition (Inverse model → Synthesizer → LPCNet):\")\n",
    "            display(Audio(repeated_wave, rate=sampling_rate))\n",
    "            print(\"Estimation (Inverse model → Direct model → LPCNet):\")\n",
    "            display(Audio(estimated_wave, rate=sampling_rate))\n",
    "            \n",
    "            # Create spectrogram visualizations\n",
    "            fig = plt.figure(figsize=(nb_frames/20, 6), dpi=120)\n",
    "            \n",
    "            ax = plt.subplot(311)\n",
    "            ax.set_title(\"original %s\" % (sound_type))\n",
    "            ax.imshow(item_cepstrum.T, origin=\"lower\")\n",
    "            \n",
    "            ax = plt.subplot(312)\n",
    "            ax.set_title(\"Repetition\")\n",
    "            ax.imshow(repeated_cepstrum.T, origin=\"lower\")\n",
    "            \n",
    "            ax = plt.subplot(313)\n",
    "            ax.set_title(\"Estimation\")\n",
    "            ax.imshow(estimated_cepstrum.T, origin=\"lower\")\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            display(fig)\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            # Convert articulatory parameters to EMA coordinates if needed\n",
    "            if art_type == \"art_params\":\n",
    "                estimated_art = synth_dataset.art_to_ema(estimated_art)\n",
    "            # item_ema = items_ema[item_name]\n",
    "            show_ema(estimated_art, reference=None, dataset=synth_dataset)\n",
    "        \n",
    "        # Create interactive widget for utterance selection\n",
    "        display(ipw.interactive(resynth_item, item_name=items_name))\n",
    "    \n",
    "    # Create interactive widget for dataset selection\n",
    "    display(ipw.interactive(show_dataset, dataset_name=agent.sound_quantizer.config[\"dataset\"][\"names\"]))\n",
    "\n",
    "# Create top-level interactive widget for agent selection\n",
    "display(ipw.interactive(show_agent, agent_path=agents_alias))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
